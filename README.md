# SMS Spam Detector ğŸš¨ğŸ“±

[![Model](https://img.shields.io/badge/ğŸ¤—-Model-yellow)](https://huggingface.co/didulanthaisuru/sms-spam-detector)
[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)

> Fine-tuned DistilBERT model for SMS spam detection achieving **99.16% accuracy** with **<6ms inference time** on GPU.

[ğŸ¤— Try the Model](https://huggingface.co/didulanthaisuru/sms-spam-detector) | [ğŸ““ Training Notebook](transformers-notebook.ipynb) | [ğŸ§ª Inference Demo](inference_example_with_huggingface.ipynb)

---

## ğŸ“Š Performance

| Metric | Ham | Spam | Overall |
|--------|-----|------|---------|
| **Precision** | 99.45% | 97.30% | 99.16% |
| **Recall** | 99.59% | 96.43% | 99.16% |
| **F1-Score** | 99.52% | 96.86% | 99.16% |
| **Support** | 724 | 112 | 836 |

**Additional Metrics:**
- âš¡ **Inference Time:** 5.90ms per message
- ğŸš€ **Throughput:** 170 messages/second
- ğŸ¯ **Accuracy:** 99.16%
- ğŸ“ˆ **Macro Avg F1:** 98.19%
- ğŸ’» **Device:** CUDA (GPU T4)

---

## ğŸ–¼ï¸ Visualizations

### Training History
![Training History](training_history.png)

### Evaluation Results
![Evaluation Results](evaluation_results.png)

---

## ğŸš€ Quick Start

### Installation

```bash
pip install transformers torch
```

### Basic Usage

```python
from transformers import pipeline

# Load the model
classifier = pipeline(
    "text-classification", 
    model="didulanthaisuru/sms-spam-detector"
)

# Predict
result = classifier("Congratulations! You've won a FREE prize. Call now!")
print(result)
# Output: [{'label': 'SPAM', 'score': 0.9876}]
```

### Detailed Usage

```python
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
import torch

# Load model and tokenizer
model_name = "didulanthaisuru/sms-spam-detector"
tokenizer = DistilBertTokenizer.from_pretrained(model_name)
model = DistilBertForSequenceClassification.from_pretrained(model_name)

def predict_spam(text):
    """Predict if a message is spam"""
    inputs = tokenizer(
        text, 
        return_tensors="pt", 
        truncation=True, 
        max_length=128,
        padding=True
    )
    
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=1)
        prediction = torch.argmax(probs, dim=1).item()
        confidence = probs[0][prediction].item()
    
    label = "SPAM" if prediction == 1 else "HAM"
    return label, confidence

# Example
text = "URGENT: Your account has been compromised. Click here now!"
label, confidence = predict_spam(text)
print(f"Prediction: {label} (Confidence: {confidence:.2%})")
```

### Batch Processing

```python
messages = [
    "Hey, are we still meeting for lunch?",
    "FREE iPhone! Click here to claim NOW!!!",
    "Your package will arrive tomorrow.",
    "WINNER! You've been selected for $1000 gift card!"
]

results = classifier(messages)

for msg, result in zip(messages, results):
    label = "ğŸš¨ SPAM" if result['label'] == 'LABEL_1' else "âœ… HAM"
    print(f"{label}: {msg[:50]}... ({result['score']:.3f})")
```

---

## ğŸ“š About the Model

### Architecture

**Base Model:** `distilbert-base-uncased`

DistilBERT is a distilled version of BERT, offering:
- ğŸ”¥ **40% smaller** than BERT (66M vs 110M parameters)
- âš¡ **60% faster** inference
- ğŸ¯ **97% of BERT's performance** retained
- ğŸ’¡ Perfect balance of speed and accuracy for production

### Training Details

| Hyperparameter | Value |
|----------------|-------|
| Base Model | `distilbert-base-uncased` |
| Task | Binary Text Classification |
| Epochs | 3 |
| Batch Size | 16 |
| Learning Rate | 2e-5 |
| Optimizer | AdamW |
| Max Sequence Length | 128 tokens |
| Weight Decay | 0.01 |
| Warmup Steps | 0 |
| Training Device | GPU T4 x2 |
| Training Time | ~20 minutes |

### Training Process

1. **Tokenization:** Text converted to token IDs using DistilBERT tokenizer
2. **Fine-tuning:** Pre-trained DistilBERT adapted for spam classification
3. **Optimization:** AdamW optimizer with linear learning rate schedule
4. **Validation:** Best model selected based on validation accuracy
5. **Evaluation:** Final testing on held-out test set

---

## ğŸ“Š Dataset Information

**Source:** [SMS Spam Collection Dataset](https://archive.ics.uci.edu/dataset/228/sms+spam+collection)

### Dataset Characteristics

| Characteristic | Value |
|----------------|-------|
| Total Messages | 5,574 |
| Ham (Legitimate) | 4,827 (86.6%) |
| Spam | 747 (13.4%) |
| Language | English |
| Average Message Length | ~80 characters |

### Data Split

- **Training Set:** 70% (3,901 messages)
- **Validation Set:** 15% (836 messages)
- **Test Set:** 15% (837 messages)

Stratified splitting ensures balanced class distribution across all sets.

### Sample Messages

**Ham Examples:**
- "Hey, are we still meeting for lunch tomorrow?"
- "Can you pick up milk on your way home?"
- "Your Amazon order has been shipped."

**Spam Examples:**
- "WINNER! You've won a $1000 gift card. Claim now!"
- "URGENT: Your account has been compromised. Click here."
- "Congratulations! FREE iPhone waiting for you!"

### Data Preprocessing

- âœ… Text normalized to lowercase (handled by tokenizer)
- âœ… Special characters preserved (important for spam patterns)
- âœ… No additional cleaning (model learns from raw text)
- âœ… Truncation to 128 tokens max
- âœ… Padding to uniform length for batching

---

## ğŸ“ Repository Structure

```
sms-spam-detector/
â”œâ”€â”€ transformers-notebook.ipynb          # Complete training pipeline
â”œâ”€â”€ inference_example_with_huggingface.ipynb  # Inference examples
â”œâ”€â”€ training_history.png                 # Training/validation curves
â”œâ”€â”€ evaluation_results.png               # Confusion matrix & ROC curve
â”œâ”€â”€ README.md                            # This file
â””â”€â”€ requirements.txt                     # Python dependencies
```

---

## ğŸ› ï¸ Technical Implementation

### Model Pipeline

```
Input Text
    â†“
Tokenization (DistilBERT Tokenizer)
    â†“
Token IDs + Attention Mask
    â†“
DistilBERT Encoder (6 layers)
    â†“
[CLS] Token Representation
    â†“
Classification Head (Linear Layer)
    â†“
Softmax Activation
    â†“
Spam/Ham Prediction
```

### Key Features

- âœ… **Transfer Learning:** Leverages pre-trained DistilBERT knowledge
- âœ… **Context Understanding:** Attention mechanism captures word relationships
- âœ… **Robust to Variations:** Handles typos, l33t speak, creative spelling
- âœ… **Fast Inference:** Optimized for real-time classification
- âœ… **Production Ready:** Deployed on HuggingFace Hub

### Technologies Used

- **PyTorch** - Deep learning framework
- **Transformers** - HuggingFace library for NLP models
- **DistilBERT** - Efficient transformer architecture
- **scikit-learn** - Metrics and evaluation
- **pandas** - Data manipulation
- **matplotlib/seaborn** - Visualization

---

## ğŸ“ˆ Detailed Results

### Classification Report

```
              precision    recall  f1-score   support

         Ham     0.9945    0.9959    0.9952       724
        Spam     0.9730    0.9643    0.9686       112

    accuracy                         0.9916       836
   macro avg     0.9837    0.9801    0.9819       836
weighted avg     0.9916    0.9916    0.9916       836
```

### Confusion Matrix

|              | Predicted Ham | Predicted Spam |
|--------------|---------------|----------------|
| **Actual Ham**  | 721 (99.6%)   | 3 (0.4%)       |
| **Actual Spam** | 4 (3.6%)      | 108 (96.4%)    |

**Interpretation:**
- âœ… **True Negatives (721):** Correctly identified legitimate messages
- âœ… **True Positives (108):** Correctly identified spam
- âš ï¸ **False Positives (3):** 3 legitimate messages marked as spam
- âš ï¸ **False Negatives (4):** 4 spam messages that got through

### Performance Analysis

**Strengths:**
- ğŸ¯ Exceptional precision on ham (99.45%) - few false alarms
- ğŸ¯ High recall on spam (96.43%) - catches most spam
- âš¡ Very fast inference (5.90ms) - real-time capable
- ğŸ’ª Robust across different spam types

**Trade-offs:**
- Slightly lower spam recall (96.43%) vs ham recall (99.59%)
- Balanced approach prioritizes user experience (minimize false positives)

---

## ğŸ¯ Use Cases

### Personal Applications
- ğŸ“± SMS spam filtering on mobile devices
- ğŸ“§ Email spam detection (with fine-tuning)
- ğŸ’¬ Chat message moderation

### Business Applications
- ğŸ¢ Corporate message filtering systems
- ğŸ›¡ï¸ Fraud detection pipelines
- ğŸ“Š Marketing campaign analysis
- ğŸ” Security alert systems

### Integration Examples

**Flask API:**
```python
from flask import Flask, request, jsonify
from transformers import pipeline

app = Flask(__name__)
classifier = pipeline("text-classification", model="didulanthaisuru/sms-spam-detector")

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    result = classifier(data['text'])[0]
    return jsonify({
        'text': data['text'],
        'prediction': result['label'],
        'confidence': result['score']
    })

if __name__ == '__main__':
    app.run(debug=True)
```

---

## ğŸ”¬ Model Evaluation

### Metrics Explanation

**Precision (97.30% for Spam):**
- When model predicts SPAM, it's correct 97.3% of the time
- Important to avoid blocking legitimate messages

**Recall (96.43% for Spam):**
- Model catches 96.43% of all spam messages
- Balances catching spam vs. false alarms

**F1-Score (96.86% for Spam):**
- Harmonic mean of precision and recall
- Overall spam detection effectiveness

### Comparison with Baselines

| Method | Accuracy | F1-Score | Inference Time |
|--------|----------|----------|----------------|
| **DistilBERT (Ours)** | **99.16%** | **96.86%** | **5.90ms** |
| Naive Bayes | 97.8% | 93.2% | 2ms |
| Logistic Regression | 98.1% | 94.5% | 1ms |
| Random Forest | 97.5% | 92.8% | 8ms |
| BERT-base | 99.2% | 97.1% | 45ms |

**Why DistilBERT?**
- âš–ï¸ Best accuracy/speed trade-off
- ğŸ¯ Near-BERT performance at 8x speed
- ğŸ’° Cost-effective for production deployment

---

## ğŸš§ Limitations

- ğŸ“ **Language:** Trained only on English SMS messages
- ğŸ”¤ **Obfuscation:** May struggle with heavily obfuscated spam (e.g., "Fr33 G!ft")
- ğŸ“ **Length:** Optimized for SMS-length messages (<160 chars)
- ğŸŒ **Domain:** Performance may vary on email or other message types
- âš ï¸ **Adversarial:** Not tested against adversarial attacks

### Known Edge Cases

```python
# May require careful tuning:
"F.R.E.E prize"           # Heavy punctuation
"Fr33 g1ft"               # L33t speak
"Prize [emoji][emoji]"    # Heavy emoji use
"à¸„à¸¸à¸“à¸Šà¸™à¸°à¸£à¸²à¸‡à¸§à¸±à¸¥"             # Non-English text
```

---

## ğŸ”® Future Improvements

### Planned Enhancements

1. **Multi-language Support**
   - Use multilingual-DistilBERT
   - Train on Spanish, French, German spam datasets

2. **Explainability**
   - Integrate LIME/SHAP for prediction explanations
   - Highlight spam-indicative words

3. **Adversarial Robustness**
   - Test against obfuscation techniques
   - Data augmentation with adversarial examples

4. **Active Learning**
   - Continuous learning from user feedback
   - Adapt to evolving spam patterns

5. **Ensemble Model**
   - Combine with rule-based filters
   - Boost edge case performance

---

## ğŸ“¦ Installation & Setup

### Prerequisites

```bash
Python >= 3.8
CUDA >= 11.0 (for GPU acceleration)
```

### Dependencies

```bash
pip install -r requirements.txt
```

**requirements.txt:**
```
transformers>=4.30.0
torch>=2.0.0
scikit-learn>=1.3.0
pandas>=2.0.0
numpy>=1.24.0
matplotlib>=3.7.0
seaborn>=0.12.0
tqdm>=4.65.0
```

### Quick Test

```python
from transformers import pipeline

# Test model loading
classifier = pipeline("text-classification", model="didulanthaisuru/sms-spam-detector")

# Test prediction
result = classifier("Test message")
print(f"Model loaded successfully! Result: {result}")
```

---

## ğŸ“– Notebooks

### Training Notebook
**File:** [`transformers-notebook.ipynb`](transformers-notebook.ipynb)

**Contains:**
- ğŸ“¥ Data loading and exploration
- ğŸ”§ Data preprocessing and tokenization
- ğŸ‹ï¸ Model training with DistilBERT
- ğŸ“Š Evaluation metrics and visualizations
- ğŸ’¾ Model saving for HuggingFace

**Run on:**
- Google Colab (Free GPU T4)
- Kaggle Notebooks
- Local with GPU

### Inference Notebook
**File:** [`inference_example_with_huggingface.ipynb`](inference_example_with_huggingface.ipynb)

**Contains:**
- ğŸ”½ Loading model from HuggingFace Hub
- ğŸ§ª Example predictions
- ğŸ“Š Batch processing examples
- âš¡ Performance benchmarking
- ğŸ’¡ Integration examples

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to:

- ğŸ› Report bugs
- ğŸ’¡ Suggest enhancements
- ğŸ”§ Submit pull requests
- ğŸ“š Improve documentation
- â­ Star this repository

---

## ğŸ“„ License

This project is licensed under the **Apache License 2.0** - see the [LICENSE](LICENSE) file for details.

The trained model is also released under Apache 2.0, allowing:
- âœ… Commercial use
- âœ… Modification
- âœ… Distribution
- âœ… Private use

---

## ğŸ™ Acknowledgments

- **Dataset:** [SMS Spam Collection](https://archive.ics.uci.edu/dataset/228/sms+spam+collection) by Almeida & Hidalgo
- **Base Model:** [DistilBERT](https://huggingface.co/distilbert-base-uncased) by HuggingFace
- **Framework:** [Transformers](https://github.com/huggingface/transformers) library
- **Platform:** [HuggingFace Hub](https://huggingface.co) for model hosting

---

## ğŸ“ Contact

**Author:** Didulantha Isuru  
**HuggingFace:** [@didulanthaisuru](https://huggingface.co/didulanthaisuru)  
**Model:** [didulanthaisuru/sms-spam-detector](https://huggingface.co/didulanthaisuru/sms-spam-detector)

For questions, issues, or collaborations:
- ğŸ’¬ Open an issue on this repository
- ğŸ“§ Contact via HuggingFace profile

---

## â­ Show Your Support

If you find this project helpful:
- â­ Star this repository
- ğŸ”„ Share with others
- ğŸ’¬ Provide feedback
- ğŸ¤ Contribute improvements

---

## ğŸ“Š Citation

If you use this model in your research or project, please cite:

```bibtex
@misc{sms-spam-detector-2025,
  author = {Didulantha Isuru},
  title = {SMS Spam Detector: Fine-tuned DistilBERT for Spam Classification},
  year = {2025},
  publisher = {HuggingFace},
  howpublished = {\url{https://huggingface.co/didulanthaisuru/sms-spam-detector}},
  note = {Accuracy: 99.16\%, F1-Score: 96.86\%}
}
```

---

<div align="center">

**Built with â¤ï¸ using PyTorch and HuggingFace Transformers**

[![HuggingFace](https://img.shields.io/badge/ğŸ¤—-HuggingFace-yellow)](https://huggingface.co/didulanthaisuru/sms-spam-detector)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red)](https://pytorch.org/)
[![Transformers](https://img.shields.io/badge/Transformers-4.30+-blue)](https://huggingface.co/docs/transformers)

*Making SMS communication safer, one message at a time* ğŸ›¡ï¸ğŸ“±

</div>
